{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This notebook trains a RNN to generate Shakespearean Sonnets and\n",
    "requires a text file containing all Shakespearean Sonnets'''\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load the raw sonnets, splicing on new lines\n",
    "data = np.loadtxt('/data/shakespeare.txt', delimiter='\\n', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the raw sonnets to create training string\n",
    "raw_X = \"\"\n",
    "for line in data:\n",
    "  line = line.strip()\n",
    "  if not line.isdigit() and len(line) >= 1:\n",
    "    for symbol in [',', '.', ';', '?', '!', ':', '(', ')']:\n",
    "      line = line.replace(symbol, '').lower()\n",
    "    raw_X += '>' + line + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create alphabet of unique characters used in sonnets\n",
    "alphabet = list(set(raw_X))\n",
    "\n",
    "# create symbol to index and index to symbol dictionaries from alphabet\n",
    "symbol_to_idx = {}\n",
    "idx_to_symbol = {}\n",
    "for i in range(len(alphabet)):\n",
    "  symbol_to_idx[alphabet[i]] = i\n",
    "  idx_to_symbol[i] = alphabet[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build training sequences of length 40 (X) and corresponding next character labels (Y)\n",
    "preX = []\n",
    "preY = []\n",
    "for i in range(0, len(raw_X) - 40, 4):\n",
    "  preX.append([symbol_to_idx[char] for char in raw_X[i : i + 40]])\n",
    "  preY.append(symbol_to_idx[raw_X[i + 40]])\n",
    "X = np.reshape(preX, (len(preX), 40, 1)) / len(alphabet)\n",
    "Y = np_utils.to_categorical(preY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "23281/23281 [==============================] - 66s 3ms/step - loss: 2.9705\n",
      "Epoch 2/60\n",
      "23281/23281 [==============================] - 65s 3ms/step - loss: 2.9376\n",
      "Epoch 3/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 2.9088\n",
      "Epoch 4/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 2.8541\n",
      "Epoch 5/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 2.7756\n",
      "Epoch 6/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 2.6724\n",
      "Epoch 7/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 2.5456\n",
      "Epoch 8/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 2.4255\n",
      "Epoch 9/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 2.3119\n",
      "Epoch 10/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 2.2123\n",
      "Epoch 11/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 2.1316\n",
      "Epoch 12/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 2.0555\n",
      "Epoch 13/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 1.9975\n",
      "Epoch 14/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 1.9404\n",
      "Epoch 15/60\n",
      "23281/23281 [==============================] - 65s 3ms/step - loss: 1.8831\n",
      "Epoch 16/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 1.8287\n",
      "Epoch 17/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 1.7800\n",
      "Epoch 18/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 1.7199\n",
      "Epoch 19/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 1.6702\n",
      "Epoch 20/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 1.6199\n",
      "Epoch 21/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 1.5606\n",
      "Epoch 22/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 1.5049\n",
      "Epoch 23/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 1.4440\n",
      "Epoch 24/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 1.3792\n",
      "Epoch 25/60\n",
      "23281/23281 [==============================] - 65s 3ms/step - loss: 1.3242\n",
      "Epoch 26/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 1.2549\n",
      "Epoch 27/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 1.1912\n",
      "Epoch 28/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 1.1275\n",
      "Epoch 29/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 1.0547\n",
      "Epoch 30/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 0.9922\n",
      "Epoch 31/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 0.9232\n",
      "Epoch 32/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.8648\n",
      "Epoch 33/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.7859\n",
      "Epoch 34/60\n",
      "23281/23281 [==============================] - 67s 3ms/step - loss: 0.7174\n",
      "Epoch 35/60\n",
      "23281/23281 [==============================] - 66s 3ms/step - loss: 0.6625\n",
      "Epoch 36/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 0.6042\n",
      "Epoch 37/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 0.5548\n",
      "Epoch 38/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 0.4976\n",
      "Epoch 39/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 0.4665\n",
      "Epoch 40/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 0.4171\n",
      "Epoch 41/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 0.3634\n",
      "Epoch 42/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 0.3112\n",
      "Epoch 43/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 0.2846\n",
      "Epoch 44/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 0.2602\n",
      "Epoch 45/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.2377\n",
      "Epoch 46/60\n",
      "23281/23281 [==============================] - 63s 3ms/step - loss: 0.2183\n",
      "Epoch 47/60\n",
      "23281/23281 [==============================] - 65s 3ms/step - loss: 0.2072\n",
      "Epoch 48/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.2588\n",
      "Epoch 49/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.2158\n",
      "Epoch 50/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.1041\n",
      "Epoch 51/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.0696\n",
      "Epoch 52/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.0500\n",
      "Epoch 53/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.0505\n",
      "Epoch 54/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.7861\n",
      "Epoch 55/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.1852\n",
      "Epoch 56/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.0759\n",
      "Epoch 57/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.0468\n",
      "Epoch 58/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.0376\n",
      "Epoch 59/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.0250\n",
      "Epoch 60/60\n",
      "23281/23281 [==============================] - 64s 3ms/step - loss: 0.0195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x63dfc3c88>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize 200 unit LSTM model and fit to X and Y\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(200, return_sequences=False))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, Y, batch_size=64, epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall i compare thee to a summer's day?\n",
      "Ooe aeauyy,\n",
      "Th t le d yhat hate the  ie messte,\n",
      "Fhen ioo and 've uet dutst tou seat'.\n",
      "For io need au wletcu sor my love ioared\n",
      "Bnt at that thou iive that t vanes brill ant feerth halee,\n",
      "And soaeven tour mq aevutes stand bpan!\n",
      "Torng myes whete the bum eress shouea!\n",
      "Though iour self in me thys brr with live,\n",
      "That shall ious sn eered vemwouee,\n",
      "The fonloundee with of siing mnd aloueeeed,\n",
      "Bnt thou art thy self thy gou wererasg boeekint oel;\n",
      "Tt wings to the mie ofe wind more that hie woell his sweet,\n",
      "Ihe soayngd san ncapl mlf the sum.\n"
     ]
    }
   ],
   "source": [
    "# set initial seed for generation as \"shall i compare thee to a summer's day \\n\"\n",
    "cur_str = []\n",
    "for char in \"shall i compare thee to a summer's day \\n\":\n",
    "    cur_str.append(symbol_to_idx[char])\n",
    "    \n",
    "# poem string to store poem lines as they are generated\n",
    "poem = \"\"\n",
    "for char in cur_str:\n",
    "    poem += idx_to_symbol[char]\n",
    "\n",
    "temperature = 0.3\n",
    "# generate enough characters (1500) to guarantee that at least 14 lines of text are generated\n",
    "for i in range(1500):\n",
    "    # normalize current character sequence to make next character prediction\n",
    "    extract = np.reshape(cur_str, (1, len(cur_str), 1)) / len(alphabet)\n",
    "    # implements temperature parameter on sampled predicions\n",
    "    pred = model.predict(extract)\n",
    "    pred = np.array(pred)\n",
    "    pred = np.log(pred) / temperature\n",
    "    pred_exp = np.exp(pred)\n",
    "    pred = pred_exp / np.sum(pred_exp)\n",
    "    # adjust p-values to ensure their sum is less than or equal to 1.0\n",
    "    adj = 0.00000001 / len(pred)\n",
    "    for i in range(len(pred)):\n",
    "        pred[i] -= adj\n",
    "    # make prediction based on multinomial distribution\n",
    "    out_idx = np.argmax(np.random.multinomial(1, np.reshape(pred, pred.size), 1))\n",
    "    # add predicted character's index to the current character sequence\n",
    "    cur_str.append(out_idx)\n",
    "    # add corresponding character to poem\n",
    "    out = idx_to_symbol[out_idx]\n",
    "    poem += out\n",
    "    # take off the first character of the current character sequence to keep its length at 40\n",
    "    cur_str = cur_str[1:]\n",
    "    \n",
    "# poem generation\n",
    "poem_lines = poem.split(sep='\\n')\n",
    "final_poem = []\n",
    "for i in range(len(poem_lines)):\n",
    "    if i == 0 and len(poem_lines[i]) > 1:\n",
    "        final_poem.append(poem_lines[i].capitalize()[0:len(poem_lines[i]) - 1] + '?')\n",
    "    elif len(final_poem) < 13 and len(poem_lines[i]) > 1:\n",
    "        if '>' in poem_lines[i]:\n",
    "            final_poem.append(poem_lines[i][1:].capitalize().replace('>', '') + np.random.choice(['', ',', '?', ';', '!', '.'], p=[0.1, 0.5, 0.1, 0.05, 0.05, 0.2]))\n",
    "        else:\n",
    "            final_poem.append(poem_lines[i][0:].capitalize().replace('>', '') + np.random.choice(['', ',', '?', ';', '!', '.'], p=[0.1, 0.5, 0.1, 0.05, 0.05, 0.2]))\n",
    "    elif len(final_poem) < 14 and len(poem_lines[i]) > 1:\n",
    "        final_poem.append(poem_lines[i][1:].capitalize().replace('>', '') + '.')\n",
    "for line in final_poem:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
